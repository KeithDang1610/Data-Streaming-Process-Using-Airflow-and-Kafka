## Context:
This project aims to build a real-time data streaming pipeline, covering each phase from data ingestion to processing and finally storage. We'll utilize a powerful stack of tools and technologies, including Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandraâ€”all neatly containerized using Docker.

## Acknowledgment:

## Project Objectives:
- Working with these data engineering tools in an individual local environment.
- Examining the workflow of streaming data processing, from extracting data via API to final storage.
- Integrating Airflow with Kafka to optimize orchestration
  
## Tools Used:
- Apache Airflow
- Apache Kafka
- Confluent Kafka
- Spark
- Docker Compose
- Cassandra

## Architecture
<p align="center">
<img src="https://github.com/user-attachments/assets/a38c34f0-9f4f-434d-ad36-8e7f3dacbe14" width=70% height=70%>
</p>

