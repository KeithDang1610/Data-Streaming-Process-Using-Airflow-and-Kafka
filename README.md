## Context:
This project aims to build a real-time data streaming pipeline, covering each phase from data ingestion to processing and finally storage. We'll utilize a powerful stack of tools and technologies, including Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandraâ€”all neatly containerized using Docker.

## Project Objectives:
- Working with these data engineering tools in an individual local environment.
- Examining the workflow of streaming data processing, from extracting data via API to final storage.
- Integrating Airflow with Kafka to optimize orchestration
- 
## Acknowledgment:
Big thanks to Youtuber **CodewithYu** who instructed and guided me to create the complex pipeline successfully. Throughout the video, he tried to provide amazing awareness, offering valuable insights that significantly enhanced my understanding. His effort to deliver clear and engaging content has been a great resource for both beginners and those looking to refine their skills in data engineering.

## Challenges:
- Some details are unclear and outdated in the latest version
- Some essential steps are missing in the video
- the requirements also conflict with the specific version being used.

## Approaches:
- Verify the dependency requirements for the specific version of Airflow (in this case, the latest version) to prevent conflicts and ensure proper compatibility.
- Choose the right images for each containers
  
## Tools Used:
- Apache Airflow
- Apache Kafka
- Confluent Kafka
- Spark
- Docker Compose
- Cassandra

## Architecture
<p align="center">
<img src="https://github.com/user-attachments/assets/a38c34f0-9f4f-434d-ad36-8e7f3dacbe14" width=70% height=70%>
</p>

